{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson3 VGG-16で画像分類をしよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "- Section0 はじめに\n",
    "- Section1 CIFAR-10\n",
    "- Section2 学習データの準備\n",
    "- Section3 VGG-16\n",
    "- Section4 ファインチューニング\n",
    "- Section5 モデルの学習、結果の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section0 はじめに\n",
    "\n",
    "こちらのソースコードはDCONでの使用環境を考え、TensorFowで使用できるようにしています。TensorFlow内のKerasで動作させているので、生のKerasでは動作できません。\n",
    "\n",
    "また、本文中では\"keras.datasets\"など、\"tensorflow\"の部分を省略した表記になっている部分がありますが、コード部分では\"tensorflow.keras.datasets\"などと\"tensorflow.keras\"モジュールを使用しているので注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 CIFAR-10\n",
    "\n",
    "まずはディープラーニング用のデータを用意します。\n",
    "\n",
    "ここでは、ディープラーニングでよく用いられるCIFAR-10を用います。\n",
    "\n",
    "CIFAR-10は、10種類（飛行機・自動車・鳥・ネコ・鹿・イヌ・カエル・馬・船・ト\n",
    "ラック）の画像が学習用に5万枚、テスト用に1万枚入ったデータセットです。\n",
    "\n",
    "各画像は縦32ピクセル×横32ピクセル×RGB3チャンネルの3次元データになります。\n",
    "\n",
    "データセットの中身は、\n",
    "\n",
    "- X : 10クラスに分類されたカラー画像（32ピクセル×32ピクセル）\n",
    "- Y : 正解ラベル（Xの画像に写っているもの）\n",
    "\n",
    "となっていて、\n",
    "\n",
    "- (X_train, Y_train) : モデルの学習用\n",
    "- (X_test, Y_test) : モデルの評価用\n",
    "\n",
    "と区別してあります。\n",
    "\n",
    "ディープラーニングでは汎化性能の向上が大切なので、学習用だけでなく評価用のデータが必要になります。\n",
    "\n",
    "評価用のデータも一緒に学習してしまうとカンニングと同じことになってしまうので、評価用は学習に用いてはいけません。\n",
    "\n",
    "そのためにCIFAR-10も学習用と評価用にあらかじめ分割しています。\n",
    "\n",
    "なお、KerasではこのCIFAR-10だけに限らず、MNISTなど機械学習で頻繁に用いられるデータセットがいくつも用意してあり、さまざまな分類学習を手軽に行えます。\n",
    "\n",
    "Kerasから使用できるデータセットの一覧はこちら（ https://keras.io/ja/datasets/ ）に載っています。\n",
    "\n",
    "これらのデータセットは**keras.datasets**以下からimportすることで使用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colabで使用する場合は以下の行を有効にしてください\n",
    "#%tensorflow_version 2.x\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 学習データの準備\n",
    "\n",
    "次に、このCIFAR-10の各画像、正解ラベルをKerasが使用しやすいように加工する必要があります。\n",
    "\n",
    "ここで特に分類タスクの際に気を付けたいことがあります。\n",
    "\n",
    "CIFAR-10では、飛行機は0、自動車は1という風に、実際のラベルは数字になっています。\n",
    "\n",
    "この数字には、数字としての大小の意味がないということです。\n",
    "\n",
    "あくまでもグループの名前であり、飛行機を表す数字が自動車を表す数字よりも大きかろうと小さかろうと、異なる数字であれば何でもいいのです。\n",
    "\n",
    "このようなグループの名前として割り振られている数字のことを**名義尺度**と呼びます。\n",
    "\n",
    "ディープラーニングのアルゴリズムでは数字の大小に意味があるものとして扱ってしまうため、名義尺度を違う形にうまいこと変換しなければなりません。\n",
    "\n",
    "ここで使用されるのが、**one-hot表現**と呼ばれるものです。\n",
    "\n",
    "全体で4クラス（0, 1, 2, 3）あるときの各クラスの表現は次の通りです。\n",
    "\n",
    "- 0 : $[1, 0, 0, 0]$\n",
    "- 1 : $[0, 1, 0, 0]$\n",
    "- 2 : $[0, 0, 1, 0]$\n",
    "- 3 : $[0, 0, 0, 1]$\n",
    "\n",
    "長さ3のベクトルを用いて、各クラスの対応する要素のみを1、他の要素を0として表現します。\n",
    "\n",
    "一般化すると、全体で$N$クラスある時、$k$番目のクラスに属するときは、\n",
    "\n",
    "$$\\underset{N}{\\underbrace{[0,\\cdots,0,\\overset{k}{\\check{1}},0,\\cdots,0]}}$$\n",
    "\n",
    "と表現します。\n",
    "\n",
    "Kerasでone-hot表現への変換を行ってくれるのが**keras.utils.to_categorical**関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 特徴量の正規化\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    " \n",
    "# クラスラベルの1-hotベクトル化\n",
    "nb_category = 10\n",
    "Y_train = to_categorical(Y_train, nb_category)\n",
    "Y_test = to_categorical(Y_test, nb_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 VGG-16\n",
    "\n",
    "VGG-16はImageNetと呼ばれる大規模画像データセットで学習された、16層からなる畳み込みニューラルネットワークモデルのことです。\n",
    "\n",
    "様々な研究で利用されている有名な学習済みモデルの1つです。\n",
    "\n",
    "層が16あることからVGG-16と名付けられていて、層の数に応じて、他にはVGG-13やVGG-19などがあります。\n",
    "\n",
    "VGG-16では出力クラスが1000あるため、入力された画像を1000のクラスに分類することができます。\n",
    "\n",
    "一番下の全結合層を取り除いて、自分で作成した全結合層と入れ替えて再学習させることにより、任意のクラス数で分類することができます。\n",
    "\n",
    "VGG-16などの学習済みモデルは**keras.applications**以下からimportすることで使用することができます。\n",
    "\n",
    "vgg16.VGG16関数の引数は以下の通りです。\n",
    "\n",
    "|引数|説明|\n",
    "|:-:|:--|\n",
    "|include_top|1000クラスに分類する全結合層を含むかどうか<br>True : 含む（元の1000分類が使いたい場合）<br>False : 含まない（出力クラスを自分で指定したい場合）|\n",
    "|weights|重みの分類<br>imagenet : ImageNetを使って学習した重み<br>None : ランダム|\n",
    "|input_tensor|モデル画像を入力する場合に使用<br>任意の画像データ : それを使用<br>None : 使用しない|\n",
    "|input_shape|入力画像の形状を指定<br>任意の形状 : それに指定<br>None : (224, 224, 3)に指定される|\n",
    "\n",
    "引数でデフォルトの場合を指定するときは省略することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "#input_tensorの定義\n",
    "image_shape = X_train.shape[1:]\n",
    "print(image_shape)\n",
    "\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16の全結合層を取り除いたので10クラスに分類するための全結合層を自分で作成し、VGG-16の後ろに連結させる必要があります。\n",
    "\n",
    "VGG16の`include_top=False`によって削除されてしまった全結合層を以下に示すので、これをお手本にしながら全結合層を自分で作成します。\n",
    "\n",
    "'''\n",
    "Layer(type)                  Output Shape              Param #\n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 25088)             0         \n",
    "_________________________________________________________________\n",
    "fc1 (Dense)                  (None, 4096)              102764544 \n",
    "_________________________________________________________________\n",
    "fc2 (Dense)                  (None, 4096)              16781312  \n",
    "_________________________________________________________________\n",
    "predictions (Dense)          (None, 1000)              4097000   \n",
    "\n",
    "'''\n",
    "\n",
    "Flatten層は3次元データである画像を畳み込んだものを1次元に変換するもので、Denseは全結合層です。\n",
    "\n",
    "ここにBatchNormalizationとDropoutを追加します。\n",
    "\n",
    "BatchNormalizationはデータの分布を0平均1分散のガウス分布に変換するもので、学習を安定化するために用いられます。\n",
    "\n",
    "Dropoutは層と層のつながりであるノードの一定割合を無効にして過学習を防ぐものです。0.1、0.3を指定すると2層の間のノードがそれぞれ全体の10%、30%が無効になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Dropout\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(BatchNormalization())\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(nb_category, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-16と全結合層を用意したら、keras.models内のModel関数で2つのモデルを連結します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# vgg16とtop_modelを連結\n",
    "model = Model(inputs=vgg16.input, outputs=top_model(vgg16.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section4 ファインチューニング\n",
    "\n",
    "VGG-16の重みはImageNet用に最適化されています。\n",
    "\n",
    "このモデルをCIFAR-10で使用するときに、全ての層について再学習させるとパラメータが多いのでとても時間がかかります。\n",
    "\n",
    "そこで、**ファインチューニング**ということを行って、必要最小限の層のもを学習させるようにします。\n",
    "\n",
    "ここで、似たものに**転移学習**というものがあるのですが、これとの違いを明確にして2つを説明します。\n",
    "\n",
    "- 転移学習\n",
    "\n",
    "    学習済みのモデルの出力層を入れ替え、その層のみを学習させて学習済みのモデルの重みをそのまま使う学習方法\n",
    "\n",
    "\n",
    "- ファインチューニング\n",
    "\n",
    "    学習済みのモデルの最後の一部分の重みを再学習させて、新しいタスクに柔軟に適合できるようにする学習方法\n",
    "\n",
    "\n",
    "一般に、ファインチューニングの方が精度が高くなります。\n",
    "\n",
    "最後の一部分のみを学習させる理由ですが、モデルの前半部分は画像の特徴の抽象的なことを主にとらえるのに対して、最後の方の層はより具体的な特徴をとらえるようになっていて、最後のだけを再学習させれば新しいものに対する具体的な特徴をとらえられることができるからです。\n",
    "\n",
    "重みを更新させない層はモデルをコンパイルする前に重みを固定する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 10)                134922    \n",
      "=================================================================\n",
      "Total params: 14,849,610\n",
      "Trainable params: 7,213,834\n",
      "Non-trainable params: 7,635,776\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 14層目までの重みを固定\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section5 モデルの学習、結果の表示\n",
    "\n",
    "いよいよモデルを学習させます。\n",
    "\n",
    "今回はfitメゾットを使用します。\n",
    "\n",
    "返り値に各エポックの学習の情報が入っているのでグラフにするために変数で受け取ります。\n",
    "\n",
    "fitの引数は以下の通りです。\n",
    "\n",
    "|引数|説明|\n",
    "|:-:|:--|\n",
    "|x|訓練データのNumpy配列（訓練データ群）|\n",
    "|y|教師データのNumpy配列|\n",
    "|batch_size|バッチサイズ<br>何枚の画像を1つのミニバッチ（画像の束）にするか<br>多いほど高速になるがメモリを多く使う|\n",
    "|epochs|学習エポック数|\n",
    "|verbose|進行状況の表示形式の指定<br>0 : 表示しない<br>1 : リアルタイムにバーでの表示（デフォルト）<br>2 : 1エポックごとに出力|\n",
    "|validation_split|訓練データに対する検証用に使用するデータの割合<br>0～1の範囲|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "# モデルの学習（時間がかかります）\n",
    "history = model.fit(X_train, Y_train, batch_size=32, validation_split=0.1, epochs=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの学習が終わったらモデルの性能の評価をしましょう。\n",
    "\n",
    "コンパイル時にmetricsにaccを指定した場合、evaluateメゾットでテストデータに対する損失、正解率が得られます。\n",
    "\n",
    "\n",
    "モデルを保存するときにはsaveメゾットを使用します。\n",
    "\n",
    "引数に保存する際のパスを指定します。\n",
    "\n",
    "保存する際の拡張子は.h5にすることが多いです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの評価\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test Score: ', score[0])\n",
    "print('Test Accuracy: ', score[1])\n",
    "\n",
    "# モデルの保存\n",
    "model.save('./CIFAR-10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各エポックの正解率を用いてグラフを作成します。\n",
    "\n",
    "グラフを作成したり、Jupyter Notebook上で画像を表示したりするときはMatplotlibを用います。\n",
    "\n",
    "中でもpyplotは最も使用されるモジュールです。\n",
    "\n",
    "見やすいグラフを作成するために、公式のマニュアルでpyplotに含まれる関数に目を通しておくことをお勧めします。\n",
    "\n",
    "参考 : https://matplotlib.org/api/pyplot_api.html\n",
    "\n",
    "なお、Jupyter Notebook上でMatplotlibの結果を表示するには、`%matplotlib inline`を冒頭で宣言する必要があります。\n",
    "\n",
    "ちなみに、%または%%から始まるJupyter Notebookに対するコマンドはマジックコマンドと呼ばれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, epoch+1), history.history['acc'], label=\"training\")\n",
    "plt.plot(range(1, epoch+1), history.history['val_acc'], label=\"validation\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020 Hibiki SAIGYO All RIghts Reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
